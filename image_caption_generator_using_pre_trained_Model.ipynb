{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRBtmcCBnqsxz6vlTUDH8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naveenand/Computer-vision/blob/main/image_caption_generator_using_pre_trained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2FgY5Y7trZm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install -q kaggle\n",
        "!pip install datasets rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 kaggle.json\n",
        "!kaggle datasets download -d adityajn105/flickr8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khkho1qcupyd",
        "outputId": "02c1e082-4bb8-4d7f-83fa-eb2bff96cc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8k.zip to /content\n",
            "100% 1.04G/1.04G [00:04<00:00, 245MB/s]\n",
            "100% 1.04G/1.04G [00:04<00:00, 228MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/flickr8k.zip"
      ],
      "metadata": {
        "id": "Uz9NIfOEvCTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "7XJQD5b3u5a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "kkO4Bo-82mnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "id": "mSrA3j48uCo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_file = '/content/captions.txt'\n",
        "image_directory = '/content/Images'"
      ],
      "metadata": {
        "id": "idqbKrbXwGfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(image_directory)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJalYEsTESLW",
        "outputId": "6997a426-69a8-46f2-ac39-3b5bbab4305e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Captions Data"
      ],
      "metadata": {
        "id": "Pa2Dc7Aa15iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def create_image_caption_mapping(caption_file):\n",
        "    # Open the caption file\n",
        "    with open(caption_file, 'r') as f:\n",
        "        next(f)\n",
        "        captions_doc = f.read()\n",
        "\n",
        "    # Create a mapping of images to captions\n",
        "    mapping = {}\n",
        "\n",
        "\n",
        "    for line in tqdm(captions_doc.split('\\n')):\n",
        "        tokens = line.split(',')\n",
        "        if len(tokens) < 2:\n",
        "            continue\n",
        "        image_id, caption = tokens[0], tokens[1:]\n",
        "        image_id = image_id.split('.')[0]\n",
        "        caption = \" \".join(caption)\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = []\n",
        "        mapping[image_id].append(caption)\n",
        "\n",
        "    return mapping"
      ],
      "metadata": {
        "id": "hSR2d8iPN9Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = create_image_caption_mapping(caption_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PFb2-EjRHOz",
        "outputId": "1184d7bf-5483-433c-84ab-428f8afc160c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40456/40456 [00:00<00:00, 157639.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMW2lRgy14vR",
        "outputId": "db3b1b52-718b-460f-afdb-379a3c3c68aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ZqfIsf14rJ",
        "outputId": "72b19f21-ec13-4413-e83b-5289875469d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Open the image\n",
        "image = Image.open(\"/content/Images/1000268201_693b08cb0e.jpg\")\n",
        "\n",
        "# Convert the image to a NumPy array\n",
        "image_array = np.array(image)\n",
        "\n",
        "# Get the shape of the array\n",
        "height, width, channels = image_array.shape\n",
        "\n",
        "# Print the shape\n",
        "print(\"Image Width:\", width)\n",
        "print(\"Image Height:\", height)\n",
        "print(\"Number of Channels:\", channels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwecpRPK8Gl4",
        "outputId": "8e58cd9c-e689-4ee8-ef8c-0edf63f0846d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Width: 375\n",
            "Image Height: 500\n",
            "Number of Channels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "2hFTJepWNSOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
      ],
      "metadata": {
        "id": "OG4Li98m9mUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image_path, text, max_length=None):\n",
        "    # Load the image\n",
        "    raw_image = Image.open(image_path)\n",
        "\n",
        "    # Conditional image captioning\n",
        "    inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
        "    out = model.generate(**inputs,max_new_tokens=max_length)\n",
        "    return processor.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "h7eubenq9Pe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_captions_for_images(image_directory, mapping, max_images=5):\n",
        "    # Initialize a dictionary to store generated captions\n",
        "    generated_captions = {}\n",
        "\n",
        "    # Variable to keep track of the number of processed images\n",
        "    processed_images = 0\n",
        "\n",
        "    for image_name, captions in mapping.items():\n",
        "        if processed_images >= max_images:\n",
        "            break  # Exit the loop if the maximum number of images is reached\n",
        "\n",
        "        image_path = os.path.join(image_directory, image_name + \".jpg\")\n",
        "        for caption in captions:\n",
        "            generated_caption = generate_caption(image_path, caption)\n",
        "            if image_name not in generated_captions:\n",
        "                generated_captions[image_name] = [generated_caption]\n",
        "            else:\n",
        "                generated_captions[image_name].append(generated_caption)\n",
        "\n",
        "        processed_images += 1  # Increment the processed image count\n",
        "\n",
        "    return generated_captions"
      ],
      "metadata": {
        "id": "2X7VpiJN9Pai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30"
      ],
      "metadata": {
        "id": "Rjx88sIG-PCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_captions = generate_captions_for_images(image_directory, mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKRTkQVo9PWM",
        "outputId": "bde49bdf-4db8-4387-cb5b-3e012e068d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 20, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 21, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 25, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_captions.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIDocqqc-Kb4",
        "outputId": "197d44e9-a28f-4acf-90f6-fbfdee1c0f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['1000268201_693b08cb0e', '1001773457_577c3a7d70', '1002674143_1b742ab4b8', '1003163366_44323f5815', '1007129816_e794419615'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(mapping.values())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiAlRAqcLH_O",
        "outputId": "4a8e5199-9198-4fbd-e0e5-5448dc82d9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              "  'A girl going into a wooden building .',\n",
              "  'A little girl climbing into a wooden playhouse .',\n",
              "  'A little girl climbing the stairs to her playhouse .',\n",
              "  'A little girl in a pink dress going into a wooden cabin .'],\n",
              " ['A black dog and a spotted dog are fighting',\n",
              "  'A black dog and a tri-colored dog playing with each other on the road .',\n",
              "  'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
              "  'Two dogs of different breeds looking at each other on the road .',\n",
              "  'Two dogs on pavement moving toward each other .'],\n",
              " ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .',\n",
              "  'A little girl is sitting in front of a large painted rainbow .',\n",
              "  'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .',\n",
              "  'There is a girl with pigtails sitting in front of a rainbow painting .',\n",
              "  'Young girl with pigtails painting outside in the grass .'],\n",
              " ['A man lays on a bench while his dog sits by him .',\n",
              "  'A man lays on the bench to which a white dog is also tied .',\n",
              "  'a man sleeping on a bench outside with a white and black dog sitting next to him .',\n",
              "  'A shirtless man lies on a park bench with his dog .',\n",
              "  'man laying on bench holding leash of dog sitting on ground'],\n",
              " ['A man in an orange hat starring at something .',\n",
              "  'A man wears an orange hat and glasses .',\n",
              "  'A man with gauges and glasses is wearing a Blitz hat .',\n",
              "  'A man with glasses is wearing a beer can crocheted hat .',\n",
              "  'The man with pierced ears is wearing glasses and an orange hat .']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ROUGE"
      ],
      "metadata": {
        "id": "rvlKBUdANIQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_metric = load_metric('rouge')"
      ],
      "metadata": {
        "id": "SmOcUMdm-KQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
      ],
      "metadata": {
        "id": "pCXQs_ePMf_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = list(mapping.values())[:5]\n",
        "rouge_metric.add(prediction =  list(generated_captions.values())[:5], reference = reference )\n",
        "score = rouge_metric.compute()\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "print('rouge_dict ', rouge_dict )\n",
        "records.append(rouge_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-jRCeFLf0d",
        "outputId": "021298f7-7086-455c-e476-8678d2ebe30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge_dict  {'rouge1': 0.9620253164556963, 'rouge2': 0.9365079365079365, 'rougeL': 0.9620253164556963, 'rougeLsum': 0.9620253164556963}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nYBSRbzP0qx",
        "outputId": "7a106a74-632b-4cc5-8f88-c43ac12887b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.9620253164556963,\n",
              " 'rouge2': 0.9365079365079365,\n",
              " 'rougeL': 0.9620253164556963,\n",
              " 'rougeLsum': 0.9620253164556963}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge_function(mapping,generated_captions):\n",
        "  rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "  reference = list(mapping.values())[:5]\n",
        "  rouge_metric.add(prediction =  list(generated_captions.values())[:5], reference = reference )\n",
        "  score = rouge_metric.compute()\n",
        "  rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "  return rouge_dict"
      ],
      "metadata": {
        "id": "5aPci_4zNcDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def functionone(caption_file,image_directory):\n",
        "  mapping = create_image_caption_mapping(caption_file)\n",
        "  generated_captions = generate_captions_for_images(image_directory, mapping)\n",
        "  rouge_function(mapping,generated_captions)\n",
        "  print('rouge score',rouge_function)"
      ],
      "metadata": {
        "id": "UMt1UQGSNbuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}